{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Going deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook tries to help you go deep with your neural net. To do so, one cannot simply increase the number of convolutional layers at will. It is important that the layers have a sufficiently high learning capacity while they should cover approximately 100% of the incoming image ([Xudong Cao 2015](https://kaggle2.blob.core.windows.net/forum-message-attachments/69182/2287/A%20practical%20theory%20for%20designing%20very%20deep%20convolutional%20neural%20networks.pdf?sv=2012-02-12&se=2015-04-19T12%3A13%3A19Z&sr=b&sp=r&sig=xXaPwlkUZjIUxRyVebSNkX9viGgDPNHHpCXJRbokxUQ%3D))."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The general approach is to try to go deep with convolutional layers. If you chain too many convolutional layers, though, the learning capacity of the layers falls too low. At this point, you have to add a max pooling layer. Use too many max pooling layers, and your image coverage grows larger than the image, which is clearly pointless. Striking the right balance while maximizing the depth of your layer is the final goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from __future__ import division\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using gpu device 0: GeForce GT 630M\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import theano\n",
    "import theano.tensor as T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from netz.layers import InputLayer, DenseLayer, OutputLayer, Conv2DCCLayer, MaxPool2DLayer, DropoutLayer\n",
    "from netz.neuralnet import NeuralNet\n",
    "from netz.costfunctions import mse, crossentropy\n",
    "from netz.nonlinearities import rectify\n",
    "from netz.updaters import Momentum, Nesterov, SGD\n",
    "from netz.visualize import plot_loss, plot_conv_weights, plot_conv_activity, plot_occlusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the MNIST data set ready. You can get it here: http://www.kaggle.com/c/digit-recognizer/data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('../data/mnist/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y = df.values[:, 0]\n",
    "X = df.values[:, 1:] / 255"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = X.astype(theano.config.floatX)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X = (X - X.mean()) / X.std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X2D = X.reshape(-1, 1, 28, 28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Useful information when going deep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is generally a good idea to use small filter sizes for your convolutional layers, generally <b>3x3</b>. The reason for this is that this allows to cover the same receptive field of the image while using less parameters that would be required if a larger filter size were used. Moreover, deeper stacks of convolutional layers are more expressive (see [here](http://cs231n.github.io/convolutional-networks/) for more)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A shallow net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers1 = [InputLayer(),\n",
    "           Conv2DCCLayer(16, (3, 3), nonlinearity=rectify),\n",
    "           MaxPool2DLayer(ds=(2, 2)),\n",
    "           Conv2DCCLayer(32, (3, 3), nonlinearity=rectify),\n",
    "           Conv2DCCLayer(32, (3, 3), nonlinearity=rectify),\n",
    "           MaxPool2DLayer(),\n",
    "           Conv2DCCLayer(32, (3, 3), nonlinearity=rectify),\n",
    "           DropoutLayer(p=0.5),\n",
    "           DenseLayer(100, nonlinearity=rectify),\n",
    "           DenseLayer(100, nonlinearity=rectify),\n",
    "           OutputLayer()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net1 = NeuralNet(layers=layers1, updater=Nesterov(), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 63306 learnable parameters\n",
      "\n",
      "\n",
      "## Layer information\n",
      "| name       | size     |   total |   cap. Y [%] |   cap. X [%] |   cov. Y [%] |   cov. X [%] |\n",
      "|:-----------|:---------|--------:|-------------:|-------------:|-------------:|-------------:|\n",
      "| input0     | 1x28x28  |     784 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "| conv2dcc0  | 16x26x26 |   10816 |       100.00 |       100.00 |        10.71 |        10.71 |\n",
      "| maxpool2d0 | 16x13x13 |    2704 |       100.00 |       100.00 |        10.71 |        10.71 |\n",
      "| conv2dcc1  | 32x11x11 |    3872 |        85.71 |        85.71 |        25.00 |        25.00 |\n",
      "| conv2dcc2  | 32x9x9   |    2592 |        54.55 |        54.55 |        39.29 |        39.29 |\n",
      "| maxpool2d1 | 32x5x5   |     800 |        54.55 |        54.55 |        39.29 |        39.29 |\n",
      "| conv2dcc3  | 32x3x3   |     288 |        63.16 |        63.16 |        67.86 |        67.86 |\n",
      "| dropout0   | 32x3x3   |     288 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "| dense0     | 100      |     100 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "| dense1     | 100      |     100 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "| output0    | 10       |      10 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "\n",
      "Explanation\n",
      "    X, Y:    image dimensions\n",
      "    cap.:    learning capacity\n",
      "    cov.:    coverage of image\n",
      "    \u001b[35mmagenta\u001b[0m: capacity too low (<1/6)\n",
      "    \u001b[36mcyan\u001b[0m:    image coverage too high (>100%)\n",
      "    \u001b[31mred\u001b[0m:     capacity too low and coverage too high\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net1.initialize(X2D, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This net is fine. The capacity never falls below 1/6, which would be 16.7%, and the coverage of the image never exceeds 100%. However, this net is not very deep, so let's try to go deeper.\n",
    "\n",
    "What we also see is the role of max pooling. If we look at 'maxpool2d1', after this layer, the capacity of the net is increased. Max pooling thus helps to increase capacity should it dip too low. However, max pooling also significantly increases the coverage of the image. So if we use max pooling too often, the coverage will quickly exceed 100% and we cannot go sufficiently deep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Not enough max poolin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers2 = [\n",
    "    InputLayer(),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify),\n",
    "    DenseLayer(100, nonlinearity=rectify),\n",
    "    DenseLayer(100, nonlinearity=rectify),\n",
    "    OutputLayer()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net2 = NeuralNet(layers=layers2, updater=Nesterov(), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 45610 learnable parameters\n",
      "\n",
      "\n",
      "## Layer information\n",
      "| name       | size     |   total |   cap. Y [%] |   cap. X [%] |   cov. Y [%] |   cov. X [%] |\n",
      "|:-----------|:---------|--------:|-------------:|-------------:|-------------:|-------------:|\n",
      "| input0     | 1x28x28  |     784 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "| conv2dcc0  | 16x26x26 |   10816 |       100.00 |       100.00 |        10.71 |        10.71 |\n",
      "| conv2dcc1  | 16x24x24 |    9216 |        60.00 |        60.00 |        17.86 |        17.86 |\n",
      "| conv2dcc2  | 16x22x22 |    7744 |        42.86 |        42.86 |        25.00 |        25.00 |\n",
      "| conv2dcc3  | 16x20x20 |    6400 |        33.33 |        33.33 |        32.14 |        32.14 |\n",
      "| conv2dcc4  | 16x18x18 |    5184 |        27.27 |        27.27 |        39.29 |        39.29 |\n",
      "| conv2dcc5  | 16x16x16 |    4096 |        23.08 |        23.08 |        46.43 |        46.43 |\n",
      "| conv2dcc6  | 16x14x14 |    3136 |        20.00 |        20.00 |        53.57 |        53.57 |\n",
      "| conv2dcc7  | 16x12x12 |    2304 |        17.65 |        17.65 |        60.71 |        60.71 |\n",
      "| \u001b[35mconv2dcc8\u001b[0m  | 16x10x10 |    1600 |        15.79 |        15.79 |        67.86 |        67.86 |\n",
      "| \u001b[35mconv2dcc9\u001b[0m  | 16x8x8   |    1024 |        14.29 |        14.29 |        75.00 |        75.00 |\n",
      "| \u001b[35mconv2dcc10\u001b[0m | 16x6x6   |     576 |        13.04 |        13.04 |        82.14 |        82.14 |\n",
      "| \u001b[35mconv2dcc11\u001b[0m | 16x4x4   |     256 |        12.00 |        12.00 |        89.29 |        89.29 |\n",
      "| \u001b[35mconv2dcc12\u001b[0m | 16x2x2   |      64 |        11.11 |        11.11 |        96.43 |        96.43 |\n",
      "| dense0     | 100      |     100 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "| dense1     | 100      |     100 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "| output0    | 10       |      10 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "\n",
      "Explanation\n",
      "    X, Y:    image dimensions\n",
      "    cap.:    learning capacity\n",
      "    cov.:    coverage of image\n",
      "    \u001b[35mmagenta\u001b[0m: capacity too low (<1/6)\n",
      "    \u001b[36mcyan\u001b[0m:    image coverage too high (>100%)\n",
      "    \u001b[31mred\u001b[0m:     capacity too low and coverage too high\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net2.initialize(X2D, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we have a very deep net but we have a problem: The lack of max pooling layers means that the capacity of the net dips below 16.7%. We need to find a better solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Too much max pooling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers3 = [\n",
    "    InputLayer(),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    MaxPool2DLayer(ds=(4, 4)),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    MaxPool2DLayer(ds=(2, 2)),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    MaxPool2DLayer(ds=(2, 2)),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    MaxPool2DLayer(ds=(2, 2)),\n",
    "    DenseLayer(100, nonlinearity=rectify),\n",
    "    DenseLayer(100, nonlinearity=rectify),\n",
    "    OutputLayer()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net3 = NeuralNet(layers=layers3, updater=Nesterov(), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 29210 learnable parameters\n",
      "\n",
      "\n",
      "## Layer information\n",
      "| name       | size     |   total |   cap. Y [%] |   cap. X [%] |   cov. Y [%] |   cov. X [%] |\n",
      "|:-----------|:---------|--------:|-------------:|-------------:|-------------:|-------------:|\n",
      "| input0     | 1x28x28  |     784 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "| conv2dcc0  | 16x28x28 |   12544 |       100.00 |       100.00 |        10.71 |        10.71 |\n",
      "| conv2dcc1  | 16x28x28 |   12544 |        60.00 |        60.00 |        17.86 |        17.86 |\n",
      "| maxpool2d0 | 16x7x7   |     784 |        60.00 |        60.00 |        17.86 |        17.86 |\n",
      "| conv2dcc2  | 16x7x7   |     784 |        92.31 |        92.31 |        46.43 |        46.43 |\n",
      "| conv2dcc3  | 16x7x7   |     784 |        57.14 |        57.14 |        75.00 |        75.00 |\n",
      "| maxpool2d1 | 16x4x4   |     256 |        57.14 |        57.14 |        75.00 |        75.00 |\n",
      "| \u001b[36mconv2dcc4\u001b[0m  | 16x4x4   |     256 |        64.86 |        64.86 |       132.14 |       132.14 |\n",
      "| \u001b[36mconv2dcc5\u001b[0m  | 16x4x4   |     256 |        45.28 |        45.28 |       189.29 |       189.29 |\n",
      "| \u001b[36mmaxpool2d2\u001b[0m | 16x2x2   |      64 |        45.28 |        45.28 |       189.29 |       189.29 |\n",
      "| \u001b[36mconv2dcc6\u001b[0m  | 16x2x2   |      64 |        56.47 |        56.47 |       303.57 |       303.57 |\n",
      "| \u001b[36mconv2dcc7\u001b[0m  | 16x2x2   |      64 |        41.03 |        41.03 |       417.86 |       417.86 |\n",
      "| \u001b[36mmaxpool2d3\u001b[0m | 16x1x1   |      16 |        41.03 |        41.03 |       417.86 |       417.86 |\n",
      "| dense0     | 100      |     100 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "| dense1     | 100      |     100 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "| output0    | 10       |      10 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "\n",
      "Explanation\n",
      "    X, Y:    image dimensions\n",
      "    cap.:    learning capacity\n",
      "    cov.:    coverage of image\n",
      "    \u001b[35mmagenta\u001b[0m: capacity too low (<1/6)\n",
      "    \u001b[36mcyan\u001b[0m:    image coverage too high (>100%)\n",
      "    \u001b[31mred\u001b[0m:     capacity too low and coverage too high\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net3.initialize(X2D, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This net uses too much max pooling for too small an image. The later layers, colored in cyan, would cover more than 100% of the image. So this network is clearly also suboptimal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A good compromise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "layers4 = [\n",
    "    InputLayer(),\n",
    "    Conv2DCCLayer(16, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    Conv2DCCLayer(32, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    Conv2DCCLayer(32, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    Conv2DCCLayer(32, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    Conv2DCCLayer(64, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    Conv2DCCLayer(64, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    Conv2DCCLayer(64, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    MaxPool2DLayer(ds=(2, 2)),\n",
    "    Conv2DCCLayer(256, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    Conv2DCCLayer(256, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    Conv2DCCLayer(256, (3, 3), nonlinearity=rectify, pad=1),\n",
    "    DenseLayer(100, nonlinearity=rectify),\n",
    "    DropoutLayer(),\n",
    "    DenseLayer(100, nonlinearity=rectify),\n",
    "    DropoutLayer(),\n",
    "    OutputLayer()\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net4 = NeuralNet(layers=layers4, updater=Nesterov(), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 6472330 learnable parameters\n",
      "\n",
      "\n",
      "## Layer information\n",
      "| name       | size      |   total |   cap. Y [%] |   cap. X [%] |   cov. Y [%] |   cov. X [%] |\n",
      "|:-----------|:----------|--------:|-------------:|-------------:|-------------:|-------------:|\n",
      "| input0     | 1x28x28   |     784 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "| conv2dcc0  | 16x28x28  |   12544 |       100.00 |       100.00 |        10.71 |        10.71 |\n",
      "| conv2dcc1  | 32x28x28  |   25088 |        60.00 |        60.00 |        17.86 |        17.86 |\n",
      "| conv2dcc2  | 32x28x28  |   25088 |        42.86 |        42.86 |        25.00 |        25.00 |\n",
      "| conv2dcc3  | 32x28x28  |   25088 |        33.33 |        33.33 |        32.14 |        32.14 |\n",
      "| conv2dcc4  | 64x28x28  |   50176 |        27.27 |        27.27 |        39.29 |        39.29 |\n",
      "| conv2dcc5  | 64x28x28  |   50176 |        23.08 |        23.08 |        46.43 |        46.43 |\n",
      "| conv2dcc6  | 64x28x28  |   50176 |        20.00 |        20.00 |        53.57 |        53.57 |\n",
      "| maxpool2d0 | 64x14x14  |   12544 |        20.00 |        20.00 |        53.57 |        53.57 |\n",
      "| conv2dcc7  | 256x14x14 |   50176 |        31.58 |        31.58 |        67.86 |        67.86 |\n",
      "| conv2dcc8  | 256x14x14 |   50176 |        26.09 |        26.09 |        82.14 |        82.14 |\n",
      "| conv2dcc9  | 256x14x14 |   50176 |        22.22 |        22.22 |        96.43 |        96.43 |\n",
      "| dense0     | 100       |     100 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "| dropout0   | 100       |     100 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "| dense1     | 100       |     100 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "| dropout1   | 100       |     100 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "| output0    | 10        |      10 |       100.00 |       100.00 |       100.00 |       100.00 |\n",
      "\n",
      "Explanation\n",
      "    X, Y:    image dimensions\n",
      "    cap.:    learning capacity\n",
      "    cov.:    coverage of image\n",
      "    \u001b[35mmagenta\u001b[0m: capacity too low (<1/6)\n",
      "    \u001b[36mcyan\u001b[0m:    image coverage too high (>100%)\n",
      "    \u001b[31mred\u001b[0m:     capacity too low and coverage too high\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net4.initialize(X2D, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With 10 convolutional layers, this network is rather deep, given the small image size. Yet the learning capacity is always suffiently large and never are more than 100% of the image covered. This could just be a good solution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note 1: The MNIST images typically don't cover the whole of the 28x28 image size. Therefore, an image coverage of less than 100% is probably very acceptable. For other image data sets such as CIFAR or ImageNet, it is recommended to cover the whole image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note 2: This analysis does not tell us how many feature maps (i.e. number of filters per convolutional layer) to use. Here we have to experiment with different values. Larger values mean that the network should learn more types of features but also increase the risk of overfitting. In general though, deeper layers (those farther down) should learn more complex features and should thus have more feature maps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More details"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to get more information by increasing the verbosity level beyond 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "net4.verbose = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "# Neural Network with 314922 learnable parameters\n",
      "\n",
      "\n",
      "## Layer information\n",
      "name        size        total    cap. Y [%]    cap. X [%]    cov. Y [%]    cov. X [%]    filter Y    filter X    field Y    field X\n",
      "----------  --------  -------  ------------  ------------  ------------  ------------  ----------  ----------  ---------  ---------\n",
      "input0      1x28x28       784        100.00        100.00        100.00        100.00          28          28         28         28\n",
      "conv2d0     16x26x26    10816        100.00        100.00         10.71         10.71           3           3          3          3\n",
      "maxpool2d0  16x13x13     2704        100.00        100.00         10.71         10.71           3           3          3          3\n",
      "conv2d1     32x11x11     3872         85.71         85.71         25.00         25.00           6           6          7          7\n",
      "conv2d2     32x9x9       2592         54.55         54.55         39.29         39.29           6           6         11         11\n",
      "conv2d3     64x7x7       3136         40.00         40.00         53.57         53.57           6           6         15         15\n",
      "conv2d4     64x5x5       1600         31.58         31.58         67.86         67.86           6           6         19         19\n",
      "conv2d5     128x3x3      1152         26.09         26.09         82.14         82.14           6           6         23         23\n",
      "conv2d6     128x1x1       128         22.22         22.22         96.43         96.43           6           6         27         27\n",
      "dense0      100           100        100.00        100.00        100.00        100.00          28          28         28         28\n",
      "dense1      100           100        100.00        100.00        100.00        100.00          28          28         28         28\n",
      "output0     10             10        100.00        100.00        100.00        100.00          28          28         28         28\n",
      "\n",
      "Explanation\n",
      "    X, Y:    image dimensions\n",
      "    cap.:    learning capacity\n",
      "    cov.:    coverage of image\n",
      "    \u001b[35mmagenta\u001b[0m: capacity too low (<1/6)\n",
      "    \u001b[36mcyan\u001b[0m:    image coverage too high (>100%)\n",
      "    \u001b[31mred\u001b[0m:     capacity too low and coverage too high\n",
      "\n"
     ]
    }
   ],
   "source": [
    "net4.initialize(X2D, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we get additional information about the real filter size of the convolutional layers, as well as their receptive field sizes. If the receptive field size grows too large compared to the real filter size, capacity dips too low. As receptive field size grows larger, more and more of the image is covered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Caveat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A caveat to the findings presented here is that capacity and coverage may not be calculated correctly if you use padding or strides other than 1 in the convolutional layers. Including this would make the calculation much more complicated. However, even if you want to use these parameters, the calculations shown here should not deviate too much and the results may still serve as a rough guideline."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
